> 对于理解、掌握分类、聚类知识很有帮助。

### 2.1 文本与处理

tokenization

##### 过滤 and （词形还原 or 词干提取）

过滤：

- 通常为移除stop words。
- 移除的是那些不常出现，或出现频率过高的词。
- 移除标点符号。
- 移除自定义的词。


词形还原：

动词到 infinite  tense， 名词复数到单数。
耗时，错误大，没有stemming应用广泛。


词干提取：

- 构建词的基本形式（build the basic forms of words）。
- Porter提出的PorterStemmer广泛使用，使用迭代的方式一步步把wrod转为他们的stem。

##### 索引条目选择（Index Term Selection）

只有那些被选中的关键词，才可以用来描述文档。（所谓的条目索引，就是减少单词呗，就是降维呗！）

简单的选择方法是熵。

### 2.2 向量空间模型（VSM）

- TF-IDF
- 欧氏距离：只用在单位向量上（正规化向量上）
- 余弦相似度：对于标准化向量/单位向量，余弦相似度与欧式距离无太大差别，二者之间存在等式。设x,y为单位向量，则：`cos(x,y) = 1 - pow(d(x,y), 2) / 2`。

### 2.3 语言预处理（Linguistic Preprocessing）

通常不进行语言预处理，但是额外的语言与处理能够增强词条的可用信息。

- POS
- Text chunking
- WSD
- Parsing


# 3 文本数据挖掘方法

### 3.1 分类

针对整个模型：
- accuracy = 正确被分类的文档数量 / 总文档数量

针对每个类：
- precision = 该类被正确分类的文档的数量 / 被分为该类的文档数量
- recall    = 该类被正确分类的文档的数量 / 该类实际的文档数量
- F-score   = 2 / (1 / recall + 1 / precision)


##### 3.1.1 索引条目选择

##### 3.1.2 朴素贝叶斯分类器

- 先验概率（prior probability）
- 后验概率（posterior probability）
- 结合朴素贝叶斯和EM（一种聚类）可以较少误差，效果还不错。

##### 3.1.3 近邻分类器

从训练集合中选择与目标文档“相似”的文档。

- k-NN分类器
- 无参数方法
- 简单
- 表现良好
- 计算开销大

##### 3.1.4 决策树分类器

数据挖掘的标准工具。

- 快速
- 可扩展

应用在文本挖掘

缺点：最终决策依赖于几个相关的term，既然是相关的词条，必然增加了误差。
改进：使用提升决策树（boosting decision trees)降低误差，备注：提升决策树与RF不同，GBDT是比较有名的提升决策树。

##### 3.1.5 支持向量机和核函数

- 适合文本分类
- 一个向量机只能进行二元分类
- 很小一部分文档是支持向量

向量机的一个重要特性是学习几乎独立与特征空间的维度，因此很适合文本分类。但是那些适合高维度的内核，容易出现过度拟合。

##### 3.1.6 分类器评估

使用的是F1-score，成绩由高到低是：
boosted tree > SVM > k-NN > DT C4.5 > NB

### 3.2 聚类

##### 3.2.1 聚类结果的评估方法

统计方法

- 平方误差均值（MSE）：类的数量大的时候，MSE效果好
- 轮廓系数（SC）：独立于类的数量

轮廓系数从-1到1。不同的轮廓系数代表的含义：
0.7 ~ 1：聚类良好，每个对象都靠近自己的簇，远离其他的簇。
0.5 ~ 0.7：对象可能被分配到近似的簇中。
0.25 ~ 0.5：存在大量的噪声，但簇依然是可辨别的。
< 0.25：无法辨别簇，聚类算法极差。


对比方法（Comparative Measures）

- 纯度：聚类结果的一致性
- 反纯度：聚类的稳定性
- Precision
- Recall
- F-Measure
- 熵：聚类结果所含的信息量的不确定性


##### 3.2.2 划分聚类

层次聚类：

1. 自顶向下：分裂聚类，每次将最分散的簇分裂（可以根据熵，SSE等度量）
2. 自底向上：凝聚聚类，每次合并相似的簇，相似性根据距离判定，距离有单链等。


k-means：

- 源自统计学。
- 文本聚类、分类也有较多使用统计的方法，k-means在文本聚类上表现良好。
- 局部最优（全局最优解释NP完全问题）


二分k均值：

- 快速，高质量


SOM：

图比《数据挖掘导论》上的要好。


EM：

- 基于统计模型
- 对于文档聚类有优异的结果


##### 3.2.3 其它聚类方法

Co-clustering：

同时对文档和词条聚类


模糊聚类：

同一个文档可属于不同的类


**文献SKK00对一些聚类方法进行了比较**


### 3.3 信息获取














